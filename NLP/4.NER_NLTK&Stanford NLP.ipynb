{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601019720665",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "' 命名实体识别（Named Entity Recognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。\\n一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。 '"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "\"\"\" 命名实体识别（Named Entity Recognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。\n",
    "一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。 \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 用nltk实现NER\n",
    "import re\n",
    "import pandas as pd \n",
    "import nltk\n",
    "\n",
    "# 将文章拆分为字符\n",
    "def parse_document(document):\n",
    "    document=re.sub('\\n',' ',document)      #用空格代替换行\n",
    "    if isinstance(document,str):\n",
    "        # 判断document是否为str类型\n",
    "        document=document\n",
    "    else:\n",
    "        raise ValueError('Document is not string!')\n",
    "    # strip() 方法用于移除字符串头尾指定的字符（默认为空格）或字符序列\n",
    "    document =document.strip()\n",
    "    sentences=nltk.sent_tokenize(document)\n",
    "    sentences=[sentence.strip() for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "# sample document\n",
    "text = \"\"\"\n",
    "FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, \n",
    "Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its \n",
    "membership now comprises 211 national associations. Member countries must each also be members of one of \n",
    "the six regional confederations into which the world is divided: Africa, Asia, Europe, North & Central America \n",
    "and the Caribbean, Oceania, and South America.\n",
    "\"\"\"\n",
    "\n",
    "# tokenize sentences\n",
    "sentences = parse_document(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "# tag sentences and use nltk's Named Entity Chunker\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "ne_chunked_sents = [nltk.ne_chunk(tagged) for tagged in tagged_sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Entity Name   Entity Type\n0       Switzerland           GPE\n1             North           GPE\n2            France           GPE\n3             Spain           GPE\n4            Sweden           GPE\n5           Denmark           GPE\n6           Belgium           GPE\n7           Germany           GPE\n8   Central America  ORGANIZATION\n9              Asia           GPE\n10      Netherlands           GPE\n11          Oceania           GPE\n12           Zürich           GPE\n13             FIFA  ORGANIZATION\n14           Africa        PERSON\n15        Caribbean      LOCATION\n16    South America           GPE\n17           Europe           GPE\n"
    }
   ],
   "source": [
    "\n",
    "# extract all named entities\n",
    "named_entities = []\n",
    "for ne_tagged_sentence in ne_chunked_sents:\n",
    "   for tagged_tree in ne_tagged_sentence:\n",
    "       # extract only chunks having NE labels\n",
    "       if hasattr(tagged_tree, 'label'):\n",
    "           entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #get NE name\n",
    "           entity_type = tagged_tree.label() # get NE category\n",
    "           named_entities.append((entity_name, entity_type))\n",
    "           # get unique named entities\n",
    "           named_entities = list(set(named_entities))\n",
    "\n",
    "# store named entities in a data frame\n",
    "entity_frame = pd.DataFrame(named_entities, columns=['Entity Name', 'Entity Type'])\n",
    "# display results\n",
    "print(entity_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 用Stanford NLP工具实现NER \"\"\"\n",
    "import re\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# 将文章拆分为字符\n",
    "def parse_document(document):\n",
    "    document=re.sub('\\n',' ',document)      #用空格代替换行\n",
    "    if isinstance(document,str):\n",
    "        # 判断document是否为str类型\n",
    "        document=document\n",
    "    else:\n",
    "        raise ValueError('Document is not string!')\n",
    "    # strip() 方法用于移除字符串头尾指定的字符（默认为空格）或字符序列\n",
    "    document =document.strip()\n",
    "    sentences=nltk.sent_tokenize(document)\n",
    "    sentences=[sentence.strip() for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "# sample document\n",
    "text = \"\"\"\n",
    "FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, \n",
    "Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its \n",
    "membership now comprises 211 national associations. Member countries must each also be members of one of \n",
    "the six regional confederations into which the world is divided: Africa, Asia, Europe, North & Central America \n",
    "and the Caribbean, Oceania, and South America.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Entity Name   Entity Type\n0                    Sweden      LOCATION\n1                    Zürich      LOCATION\n2                      Asia      LOCATION\n3             South America      LOCATION\n4                    France      LOCATION\n5                   Germany      LOCATION\n6                   Belgium      LOCATION\n7                      1904          DATE\n8                   Oceania      LOCATION\n9           the Netherlands      LOCATION\n10                     FIFA  ORGANIZATION\n11                   Africa      LOCATION\n12  North & Central America  ORGANIZATION\n13                Caribbean      LOCATION\n14              Switzerland      LOCATION\n15                    Spain      LOCATION\n16                  Denmark      LOCATION\n17                   Europe      LOCATION\n"
    }
   ],
   "source": [
    "sentences = parse_document(text)\n",
    "tokenized_sentences=[nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "# load stanford NER\n",
    "sn = StanfordNERTagger('D://Files/stanford-ner-4.0.0/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "                       path_to_jar='D://Files/stanford-ner-4.0.0/stanford-ner.jar')\n",
    "\n",
    "# tag sentences\n",
    "ne_annotated_sentences = [sn.tag(sent) for sent in tokenized_sentences]\n",
    "# print(ne_annotated_sentences)\n",
    "# extract named entities\n",
    "named_entities=[]\n",
    "for sentence in ne_annotated_sentences:\n",
    "    temp_entity_name=''\n",
    "    temp_named_entity=None\n",
    "    for term,tag in sentence:\n",
    "        # get terms with NE tags\n",
    "        if tag!='O':\n",
    "            temp_entity_name=' '.join([temp_entity_name,term]).strip()  #get NE name\n",
    "            temp_named_entity=(temp_entity_name,tag)    # get NE and its category\n",
    "        else:\n",
    "            if temp_named_entity:\n",
    "                named_entities.append(temp_named_entity)\n",
    "                temp_entity_name=''\n",
    "                temp_named_entity=None\n",
    "\n",
    "# get unique named entities\n",
    "named_entities = list(set(named_entities))\n",
    "# store named entities in a data frame\n",
    "entity_frame = pd.DataFrame(named_entities, columns=['Entity Name', 'Entity Type'])\n",
    "# display results\n",
    "print(entity_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 可以看到，在Stanford NER的帮助下，NER的实现效果较好，将Africa识别为LOCATION，将1904识别为时间（这在NLTK中没有识别出来），但还是对North & Central America识别有误，将其识别为ORGANIZATION。\n",
    "  值得注意的是，并不是说Stanford NER一定会比NLTK NER的效果好，两者针对的对象，预料，算法可能有差异，因此，需要根据自己的需求决定使用什么工具。\n",
    "\n",
    "作者：山阴少年\n",
    "链接：https://www.jianshu.com/p/16e1f6a7aaef\n",
    "来源：简书\n",
    "著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 \"\"\""
   ]
  }
 ]
}